"""
Train Specialized Respiratory Risk Classifier
Filters vitals dataset to respiratory-specific features

Expected Accuracy: 91-94% (respiratory issues have distinct patterns)
Training Time: ~2 minutes on RTX 3050
Model Size: 0.7k parameters (ultra-lightweight for edge deployment)
"""

import torch
import torch.optim as optim
import torch.nn as nn
import pandas as pd
import numpy as np
import os
import sys
import json
from datetime import datetime
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix
)
import joblib

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from ml.models.specialized_classifiers import RespiratoryRiskClassifier

# Configuration
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BATCH_SIZE = 256
LEARNING_RATE = 1e-3
EPOCHS = 100
EARLY_STOPPING_PATIENCE = 15
SEED = 42

torch.manual_seed(SEED)
np.random.seed(SEED)

print("=" * 80)
print("TRAINING RESPIRATORY RISK CLASSIFIER")
print("=" * 80)
print(f"\nüñ•Ô∏è  Device: {torch.cuda.get_device_name(0) if DEVICE == 'cuda' else 'CPU'}")
print(f"üìä Config: {EPOCHS} epochs, LR={LEARNING_RATE}, Batch={BATCH_SIZE}\n")

# Load dataset
csv_path = os.path.join(os.path.dirname(__file__), '..', '..', 'data', 'human_vital_signs_dataset_2024.csv')
print(f"üìÇ Loading: {os.path.basename(csv_path)}")
df = pd.read_csv(csv_path)
print(f"   ‚úì Loaded {len(df):,} patient records\n")

# RESPIRATORY-SPECIFIC FEATURES (expanded for better accuracy)
RESP_FEATURES = [
    'Oxygen Saturation',        # Primary respiratory indicator
    'Respiratory Rate',         # Primary respiratory indicator
    'Body Temperature',         # Fever indicates infection/inflammation
    'Heart Rate',              # Tachycardia common in respiratory distress
    'Age',                     # Elderly at higher respiratory risk
    'Systolic Blood Pressure', # Hypotension in severe respiratory failure
    'Diastolic Blood Pressure',# Blood pressure changes with hypoxia
    'Derived_BMI',            # Obesity major risk factor for respiratory disease
    'Weight (kg)',            # Weight impacts lung capacity
    'Derived_HRV'             # Reduced HRV in respiratory compromise
]

print(f"ü´Å Respiratory Features ({len(RESP_FEATURES)}):")
for i, feat in enumerate(RESP_FEATURES, 1):
    if feat in df.columns:
        feat_min, feat_max = df[feat].min(), df[feat].max()
        feat_mean = df[feat].mean()
        print(f"   {i}. {feat:25s} ‚Üí [{feat_min:7.2f}, {feat_max:7.2f}] (Œº={feat_mean:7.2f})")
    else:
        print(f"   {i}. {feat:25s} ‚Üí ‚ö†Ô∏è  MISSING")

missing_features = [f for f in RESP_FEATURES if f not in df.columns]
if missing_features:
    print(f"\n‚ùå ERROR: Missing features: {missing_features}")
    sys.exit(1)

# Prepare data
X = df[RESP_FEATURES].values.astype(np.float32)
y = (df['Risk Category'] == 'High Risk').astype(int).values

print(f"\nüìà Dataset Statistics:")
print(f"   Total samples: {len(X):,}")
print(f"   Low Risk:  {(y == 0).sum():,} ({(y == 0).sum()/len(y)*100:.1f}%)")
print(f"   High Risk: {(y == 1).sum():,} ({(y == 1).sum()/len(y)*100:.1f}%)")

# Train/Val/Test splits
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.30, random_state=SEED, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp
)

print(f"\nüîÄ Data Splits:")
print(f"   Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)")
print(f"   Val:   {len(X_val):,} ({len(X_val)/len(X)*100:.1f}%)")
print(f"   Test:  {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)")

# Normalize
print(f"\nüîß Normalizing features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
X_test_scaled = scaler.transform(X_test)

if np.isnan(X_train_scaled).any():
    print("   ‚ö†Ô∏è  WARNING: NaN detected!")
else:
    print("   ‚úì No NaN/Inf")

scaler_path = os.path.join(os.path.dirname(__file__), 'resp_scaler.joblib')
joblib.dump(scaler, scaler_path)
print(f"   ‚úì Saved: {os.path.basename(scaler_path)}")

# DataLoaders
train_dataset = TensorDataset(
    torch.from_numpy(X_train_scaled).float(),
    torch.from_numpy(y_train).float().unsqueeze(1)
)
val_dataset = TensorDataset(
    torch.from_numpy(X_val_scaled).float(),
    torch.from_numpy(y_val).float().unsqueeze(1)
)
test_dataset = TensorDataset(
    torch.from_numpy(X_test_scaled).float(),
    torch.from_numpy(y_test).float().unsqueeze(1)
)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# Model (increased capacity for 10 features)
print(f"\nüèóÔ∏è  Model Architecture:")
model = RespiratoryRiskClassifier(
    input_dim=len(RESP_FEATURES),
    hidden_dims=[64, 32, 16],  # Increased from [32,16,8]
    dropout=0.3  # Slightly higher regularization
).to(DEVICE)

print(model)
total_params = sum(p.numel() for p in model.parameters())
print(f"   Parameters: {total_params:,}")
print(f"   Size: {total_params * 4 / 1024:.2f} KB")

# Training setup with class weighting
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Calculate positive class weight for imbalance
num_low_risk = (y_train == 0).sum()
num_high_risk = (y_train == 1).sum()
pos_weight = torch.tensor([num_low_risk / num_high_risk]).to(DEVICE)

criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

print(f"\n‚öñÔ∏è  Class Balance:")
print(f"   Positive weight (High Risk): {pos_weight.item():.3f}")
print(f"   This compensates for {num_low_risk:,} Low vs {num_high_risk:,} High samples")

print(f"\nüöÄ Training...")
print("=" * 80)

best_val_auc = 0.0
best_epoch = 0
patience_counter = 0

for epoch in range(1, EPOCHS + 1):
    # Train
    model.train()
    train_loss = 0.0
    
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)
        
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item() * batch_X.size(0)
    
    train_loss /= len(train_loader.dataset)
    
    # Validate
    model.eval()
    val_loss = 0.0
    all_preds, all_probs, all_labels = [], [], []
    
    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            batch_X, batch_y = batch_X.to(DEVICE), batch_y.to(DEVICE)
            
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            val_loss += loss.item() * batch_X.size(0)
            
            # Apply sigmoid for BCEWithLogitsLoss
            probs = torch.sigmoid(outputs).cpu().numpy()
            preds = (probs > 0.5).astype(int)
            
            all_probs.extend(probs.flatten())
            all_preds.extend(preds.flatten())
            all_labels.extend(batch_y.cpu().numpy().flatten())
    
    val_loss /= len(val_loader.dataset)
    val_accuracy = accuracy_score(all_labels, all_preds)
    val_auc = roc_auc_score(all_labels, all_probs)
    
    if epoch % 5 == 0 or epoch == 1:
        print(f"Epoch {epoch:3d}/{EPOCHS} ‚îÇ "
              f"Train: {train_loss:.4f} ‚îÇ "
              f"Val: {val_loss:.4f} ‚îÇ "
              f"Acc: {val_accuracy:.4f} ‚îÇ "
              f"AUC: {val_auc:.4f}")
    
    # Save best
    if val_auc > best_val_auc:
        best_val_auc = val_auc
        best_epoch = epoch
        patience_counter = 0
        
        checkpoint_path = os.path.join(os.path.dirname(__file__), 'resp_classifier_best.pt')
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'val_auc': val_auc,
            'val_accuracy': val_accuracy,
            'feature_cols': RESP_FEATURES,
            'scaler_mean': scaler.mean_.tolist(),
            'scaler_std': scaler.scale_.tolist(),
            'config': {
                'input_dim': len(RESP_FEATURES),
                'hidden_dims': [32, 16, 8],
                'dropout': 0.25
            }
        }, checkpoint_path)
        
        if epoch > 1:
            print(f"   ‚úÖ Best! AUC={val_auc:.4f}")
    else:
        patience_counter += 1
    
    if patience_counter >= EARLY_STOPPING_PATIENCE:
        print(f"\n‚èπÔ∏è  Early stop at epoch {epoch}")
        break

print("\n" + "=" * 80)
print("‚úÖ TRAINING COMPLETE")
print("=" * 80)

# Test evaluation
checkpoint = torch.load(checkpoint_path)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

print(f"\nüìä FINAL TEST EVALUATION")
print("=" * 80)

all_test_preds, all_test_probs, all_test_labels = [], [], []

with torch.no_grad():
    for batch_X, batch_y in test_loader:
        batch_X = batch_X.to(DEVICE)
        outputs = model(batch_X)
        
        # Apply sigmoid for BCEWithLogitsLoss
        probs = torch.sigmoid(outputs).cpu().numpy()
        preds = (probs > 0.5).astype(int)
        
        all_test_probs.extend(probs.flatten())
        all_test_preds.extend(preds.flatten())
        all_test_labels.extend(batch_y.numpy().flatten())

test_accuracy = accuracy_score(all_test_labels, all_test_preds)
test_precision = precision_score(all_test_labels, all_test_preds, zero_division=0)
test_recall = recall_score(all_test_labels, all_test_preds, zero_division=0)
test_f1 = f1_score(all_test_labels, all_test_preds, zero_division=0)
test_auc = roc_auc_score(all_test_labels, all_test_probs)
conf_matrix = confusion_matrix(all_test_labels, all_test_preds)

print(f"\nüìà Test Metrics:")
print(f"   Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
print(f"   Precision: {test_precision:.4f}")
print(f"   Recall:    {test_recall:.4f}")
print(f"   F1:        {test_f1:.4f}")
print(f"   AUC:       {test_auc:.4f}")

print(f"\nüìã Confusion Matrix:")
print(f"               Low    High")
print(f"   Low      {conf_matrix[0,0]:5d}  {conf_matrix[0,1]:5d}")
print(f"   High     {conf_matrix[1,0]:5d}  {conf_matrix[1,1]:5d}")

tn, fp, fn, tp = conf_matrix.ravel()
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)

print(f"\nüéØ Clinical Metrics:")
print(f"   Sensitivity: {sensitivity:.4f} ({sensitivity*100:.1f}%)")
print(f"   Specificity: {specificity:.4f} ({specificity*100:.1f}%)")

# Save results
results = {
    'model_type': 'RespiratoryRiskClassifier',
    'training_date': datetime.now().isoformat(),
    'features': RESP_FEATURES,
    'test_metrics': {
        'accuracy': float(test_accuracy),
        'precision': float(test_precision),
        'recall': float(test_recall),
        'f1_score': float(test_f1),
        'roc_auc': float(test_auc),
        'sensitivity': float(sensitivity),
        'specificity': float(specificity)
    },
    'confusion_matrix': conf_matrix.tolist(),
    'model_size_kb': total_params * 4 / 1024
}

results_path = os.path.join(os.path.dirname(__file__), 'resp_training_results.json')
with open(results_path, 'w') as f:
    json.dump(results, f, indent=2)

print(f"\nüíæ Saved:")
print(f"   Model: {os.path.basename(checkpoint_path)}")
print(f"   Results: {os.path.basename(results_path)}")

print("\n" + "=" * 80)
print("üéØ ACCURACY VALIDATION")
print("=" * 80)
print(f"‚úÖ Target: ‚â•91%")
print(f"üìä Achieved: {test_accuracy*100:.2f}%")
print(f"{'‚úÖ PASS' if test_accuracy >= 0.91 else '‚ö†Ô∏è  BELOW TARGET'}")
print("=" * 80)
